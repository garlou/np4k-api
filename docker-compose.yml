version: '3.8'

services:
  article-parser-api:
    build: .
    ports:
      - "3002:3002"
    env_file:
      - secrets.env
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    environment:
      - GUNICORN_TIMEOUT=60
      - GUNICORN_WORKERS=2
    depends_on:
      - ollama
    networks:
      - llm-network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull llama3.2 && \
      wait"]

  # ollama:  # New service for running the Dockerfile in /ollama
  #   image: ollama/ollama:latest
  #   pull_policy: always
  #   container_name: ollama
  #   ports: ["11434:11434"] # will be accessible in http://localhost:11434
  #   volumes:
  #     - ./model_files:/model_files  # Mount the directory with the trained model
  #   tty: true
  #   command: >
  #     ollama serve &&
  #     ollama pull llama3.2

networks:
  llm-network:
    driver: bridge

volumes:
  ollama_data: